# Light-TTS æœ¬åœ°å¤š Worker éƒ¨ç½²æŒ‡å— (ä¿®å¤ç‰ˆ)

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨å•å°æœåŠ¡å™¨ä¸Šéƒ¨ç½² 1ä¸ª LLM + 2-3ä¸ª Flow+HiFi-GAN Worker çš„é…ç½®ã€‚**æ³¨æ„ï¼šéœ€è¦å…ˆä¿®å¤ decode_process_num é™åˆ¶é—®é¢˜**ï¼ˆå‚è€ƒ `decode-process-limit-analysis.md`ï¼‰ã€‚

## ğŸš¨ éƒ¨ç½²å‰å¿…è¯»

### ä¿®å¤é™åˆ¶é—®é¢˜

åœ¨éƒ¨ç½²å¤šworkerä¹‹å‰ï¼Œå¿…é¡»å…ˆä¿®å¤ä»£ç ä¸­çš„é™åˆ¶ï¼š

1. **ä¿®æ”¹ api_start.py**ï¼š
   ```python
   # æ³¨é‡Šæˆ–åˆ é™¤ç¬¬111-112è¡Œçš„é™åˆ¶
   # num_loras = 1
   # assert args.decode_process_num <= num_loras

   # ä¿®æ”¹ç¬¬142-143è¡Œçš„ç«¯å£åˆ†é…
   # åŸä»£ç ï¼š
   # tts_decode_ports = can_use_ports[0 : num_loras]
   # ä¿®æ”¹ä¸ºï¼š
   tts_decode_ports = can_use_ports[0 : args.decode_process_num]
   ```

2. **éªŒè¯ä¿®å¤**ï¼š
   ```bash
   # åº”è¯¥å¯ä»¥æˆåŠŸå¯åŠ¨å¤šä¸ªworkers
   python -m light_tts.server.api_server --decode_process_num 3 --model_dir /path/to/model
   ```

## æ¶æ„æ¦‚è§ˆ

### æœ¬åœ°å¤šè¿›ç¨‹æ¶æ„ï¼ˆä¿®å¤åï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HTTP Server                          â”‚
â”‚              (FastAPI + Gunicorn)                      â”‚
â”‚                    Workers: 3                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HttpServerManager                          â”‚
â”‚                   (è¿›ç¨‹é—´åè°ƒ)                           â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚           â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encode  â”‚ â”‚    LLM    â”‚   â”‚  Decode 1     â”‚ â”‚ Decode 2 â”‚
â”‚Process 1â”‚ â”‚ Process 1 â”‚   â”‚ Process 1     â”‚ â”‚ Process 2 â”‚
â”‚         â”‚ â”‚ (GPU 0)   â”‚   â”‚ (GPU 1)       â”‚ â”‚ (GPU 2)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµè®¾è®¡

```
HTTP Request â†’ HttpServer â†’ Encode â†’ LLM â†’ (è´Ÿè½½å‡è¡¡) â†’ Decode Workers
                                          â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ Decode 1    â”‚ â† GPU 1
                                    â”‚ (Port 8084) â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ Decode 2    â”‚ â† GPU 2
                                    â”‚ (Port 8085) â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è¿›ç¨‹èŒè´£

1. **HTTP Server**ï¼šå¤„ç†å¤–éƒ¨è¯·æ±‚ï¼Œä½¿ç”¨ Gunicorn å¤šè¿›ç¨‹æé«˜å¹¶å‘
2. **Encode Process**ï¼šæ–‡æœ¬é¢„å¤„ç†å’Œè¯´è¯äººç‰¹å¾æå–ï¼ˆCPUå¯†é›†ï¼‰
3. **LLM Process**ï¼šè¯­éŸ³ token ç”Ÿæˆï¼ˆGPU å¯†é›†ï¼‰
4. **Decode Processes**ï¼šFlow+HiFi-GAN éŸ³é¢‘åˆæˆï¼ˆGPU å¯†é›†ï¼Œå¤šè¿›ç¨‹ï¼‰

## éƒ¨ç½²é…ç½®

### åŸºç¡€é…ç½®ï¼ˆ2ä¸ª Decode Workersï¼‰

```bash
#!/bin/bash
# scripts/start-2workers.sh

# ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# æ¨¡å‹è·¯å¾„
MODEL_DIR="./pretrained_models/CosyVoice2-0.5B-latest"

# å¯åŠ¨å‘½ä»¤
python -m light_tts.server.api_server \
  --model_dir ${MODEL_DIR} \
  --host 0.0.0.0 \
  --port 8080 \
  --encode_process_num 1 \
  --encode_paral_num 4 \
  --decode_process_num 2 \
  --decode_paral_num 2 \
  --decode_max_batch_size 4 \
  --gpt_paral_num 100 \
  --gpt_paral_step_num 200 \
  --max_total_token_num 65536 \
  --max_req_total_len 32768 \
  --batch_max_tokens 8192 \
  --router_max_wait_tokens 8 \
  --cache_capacity 200 \
  --load_trt True \
  --mode triton_flashdecoding \
  --httpserver_workers 3
```

### é«˜æ€§èƒ½é…ç½®ï¼ˆ3ä¸ª Decode Workersï¼‰

```bash
#!/bin/bash
# scripts/start-3workers.sh

# ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# æ¨¡å‹è·¯å¾„
MODEL_DIR="./pretrained_models/CosyVoice2-0.5B-latest"

# å¯åŠ¨å‘½ä»¤
python -m light_tts.server.api_server \
  --model_dir ${MODEL_DIR} \
  --host 0.0.0.0 \
  --port 8080 \
  --encode_process_num 1 \
  --encode_paral_num 6 \
  --decode_process_num 3 \
  --decode_paral_num 2 \
  --decode_max_batch_size 3 \
  --gpt_paral_num 150 \
  --gpt_paral_step_num 300 \
  --max_total_token_num 98304 \
  --max_req_total_len 32768 \
  --batch_max_tokens 12288 \
  --router_max_wait_tokens 6 \
  --cache_capacity 300 \
  --load_trt True \
  --mode triton_flashdecoding,triton_int8kv \
  --httpserver_workers 4 \
  --log_stats_interval 10
```

### Gunicorn ç”Ÿäº§é…ç½®

```python
# wsgi_wrapper.py
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ä¿®æ”¹å¯åŠ¨å‚æ•°
if len(sys.argv) > 1:
    pass
else:
    # é»˜è®¤ç”Ÿäº§é…ç½®
    sys.argv.extend([
        "--model_dir", "./pretrained_models/CosyVoice2-0.5B-latest",
        "--decode_process_num", "3",
        "--decode_max_batch_size", "3",
        "--gpt_paral_num", "150",
        "--load_trt", "True",
        "--httpserver_workers", "4"
    ])

# å¯åŠ¨åº”ç”¨
from light_tts.server.api_http import app
```

```bash
#!/bin/bash
# run.sh

# æ€§èƒ½ä¼˜åŒ–ç¯å¢ƒå˜é‡
export OMP_NUM_THREADS=8
export OPENBLAS_NUM_THREADS=8
export MKL_NUM_THREADS=8
export CUDA_LAUNCH_BLOCKING=0
export NCCL_DEBUG=WARN

# Gunicorn é…ç½®
WORKERS=4                    # HTTP worker è¿›ç¨‹æ•°
WORKER_CLASS=uvicorn.workers.UvicornWorker
HOST=0.0.0.0
PORT=8080
TIMEOUT=300
KEEPALIVE=5

# å¯åŠ¨å‘½ä»¤
exec gunicorn -w $WORKERS \
    -k $WORKER_CLASS \
    -b $HOST:$PORT \
    --timeout $TIMEOUT \
    --keep-alive $KEEPALIVE \
    --access-logfile - \
    --error-logfile - \
    --preload \
    wsgi_wrapper:app
```

## GPU é…ç½®ä¼˜åŒ–

### GPU èµ„æºåˆ†é…

```python
# è‡ªåŠ¨ GPU åˆ†é…é€»è¾‘ï¼ˆå·²åœ¨ä»£ç ä¸­å®ç°ï¼‰
def get_gpu_id(decode_proc_index, total_gpus):
    """ä¸º Decode è¿›ç¨‹åˆ†é… GPU"""
    return decode_proc_index % total_gpus

# ç¤ºä¾‹åˆ†é…æ–¹æ¡ˆ
# 3 ä¸ª GPU, 3 ä¸ª Decode Workers:
# - Decode 0 -> GPU 0 (LLM ä¹Ÿåœ¨ GPU 0)
# - Decode 1 -> GPU 1
# - Decode 2 -> GPU 2
```

### å†…å­˜ä¼˜åŒ–

```python
# config/memory_config.py
import torch

def optimize_gpu_memory():
    """ä¼˜åŒ– GPU å†…å­˜ä½¿ç”¨"""
    # å¯ç”¨å†…å­˜æ˜ å°„
    torch.multiprocessing.set_sharing_strategy('file_system')

    # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
    torch.cuda.set_per_process_memory_fraction(0.9)  # ä½¿ç”¨90% GPUå†…å­˜

    # å¯ç”¨ç¼“å­˜åˆ†é…å™¨
    torch.cuda.empty_cache()

    # è®¾ç½®å†…å­˜æ± 
    torch.cuda.memory.set_per_process_memory_fraction(0.95)

# GPU å†…å­˜ç›‘æ§
def monitor_gpu_memory():
    """ç›‘æ§ GPU å†…å­˜ä½¿ç”¨"""
    for i in range(torch.cuda.device_count()):
        allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
        reserved = torch.cuda.memory_reserved(i) / 1024**3   # GB
        total = torch.cuda.get_device_properties(i).total_memory / 1024**3

        print(f"GPU {i}: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)")
```

## æ€§èƒ½è°ƒä¼˜

### å‚æ•°è¯´æ˜ä¸è°ƒä¼˜å»ºè®®

| å‚æ•° | é»˜è®¤å€¼ | å»ºè®®å€¼ | è¯´æ˜ |
|------|--------|--------|------|
| `decode_process_num` | 1 | 2-3 | Decode è¿›ç¨‹æ•°ï¼Œæ ¹æ® GPU æ•°é‡è®¾ç½® |
| `decode_max_batch_size` | 1 | 2-4 | æ‰¹å¤§å°ï¼Œå½±å“ååå’Œå»¶è¿Ÿ |
| `decode_paral_num` | 1 | 2-4 | æ¯ä¸ª Decode è¿›ç¨‹çš„å¹¶è¡Œåº¦ |
| `gpt_paral_num` | 50 | 100-200 | LLM æ¨ç†å¹¶è¡Œåº¦ |
| `batch_max_tokens` | None | 8192-16384 | æ‰¹å¤„ç†æœ€å¤§ token æ•° |
| `router_max_wait_tokens` | 8 | 4-8 | è°ƒåº¦ç­‰å¾…æ—¶é—´ï¼Œè¶Šå°å»¶è¿Ÿè¶Šä½ |

### åœºæ™¯ä¼˜åŒ–é…ç½®

#### é«˜ååé…ç½®ï¼ˆé•¿æ–‡æœ¬ï¼‰

```bash
# ä¼˜åŒ–æ‰¹å¤„ç†ï¼Œæé«˜ååé‡
--decode_process_num 3
--decode_max_batch_size 4
--gpt_paral_num 200
--batch_max_tokens 16384
--router_max_wait_tokens 12
```

#### ä½å»¶è¿Ÿé…ç½®ï¼ˆçŸ­æ–‡æœ¬/å®æ—¶ï¼‰

```bash
# å‡å°‘æ‰¹å¤„ç†ï¼Œé™ä½å»¶è¿Ÿ
--decode_process_num 3
--decode_max_batch_size 2
--gpt_paral_num 100
--batch_max_tokens 4096
--router_max_wait_tokens 4
--gpt_paral_step_num 100
```

#### å†…å­˜å—é™é…ç½®

```bash
# å‡å°‘å†…å­˜ä½¿ç”¨
--decode_process_num 2
--decode_max_batch_size 2
--max_total_token_num 32768
--mode ppl_int8kv  # å¯ç”¨ KV Cache é‡åŒ–
```

## ç›‘æ§ä¸è¯Šæ–­

### æ€§èƒ½ç›‘æ§è„šæœ¬

```python
# scripts/monitor.py
import psutil
import torch
import time
import requests

def monitor_system():
    """ç›‘æ§ç³»ç»Ÿèµ„æº"""
    while True:
        # CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)

        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()

        # GPU ä½¿ç”¨ç‡
        gpu_stats = []
        for i in range(torch.cuda.device_count()):
            gpu_stats.append({
                'id': i,
                'memory_used': torch.cuda.memory_allocated(i) / 1024**3,
                'memory_total': torch.cuda.get_device_properties(i).total_memory / 1024**3,
                'utilization': get_gpu_utilization(i)
            })

        # TPS (æ¯ç§’å¤„ç†è¯·æ±‚æ•°)
        tps = get_current_tps()

        # æ‰“å°ç›‘æ§ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"CPU: {cpu_percent:.1f}%")
        print(f"Memory: {memory.percent:.1f}% ({memory.used/1024**3:.1f}GB)")

        for gpu in gpu_stats:
            print(f"GPU {gpu['id']}: {gpu['utilization']:.1f}% | "
                  f"Memory: {gpu['memory_used']:.1f}/{gpu['memory_total']:.1f}GB")

        print(f"TPS: {tps:.1f}")

        time.sleep(5)

def get_gpu_utilization(gpu_id):
    """è·å– GPU åˆ©ç”¨ç‡ï¼ˆéœ€è¦ nvidia-ml-pyï¼‰"""
    try:
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_id)
        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
        return util.gpu
    except:
        return 0

def get_current_tps():
    """è·å–å½“å‰ TPS"""
    try:
        # ä» Prometheus è·å–æŒ‡æ ‡
        response = requests.get('http://localhost:8080/metrics')
        # è§£ææŒ‡æ ‡è¿”å› TPS
        return 0
    except:
        return 0

if __name__ == "__main__":
    monitor_system()
```

### æ—¥å¿—åˆ†æ

```bash
# æå–æ€§èƒ½æ—¥å¿—
grep "cost_time" logs/light-tts.log | \
  awk '{print $NF}' | \
  sed 's/ms//' | \
  sort -n | \
  awk 'BEGIN{count=0; sum=0} {count++; sum+=$1; arr[NR]=$1}
  END{print "Count:", count;
       print "Avg:", sum/count "ms";
       print "P50:", arr[int(NR*0.5)] "ms";
       print "P90:", arr[int(NR*0.9)] "ms";
       print "P99:", arr[int(NR*0.99)] "ms"}'
```

## æ•…éšœå¤„ç†

### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### 1. GPU å†…å­˜ä¸è¶³

```bash
# é”™è¯¯ï¼šCUDA out of memory
# è§£å†³æ–¹æ¡ˆï¼š
# 1. å‡å°‘æ‰¹å¤§å°
--decode_max_batch_size 2

# 2. å¯ç”¨é‡åŒ–
--mode ppl_int8kv

# 3. å‡å°‘ token å®¹é‡
--max_total_token_num 32768
```

#### 2. è¿›ç¨‹å¡æ­»

```bash
# æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
ps aux | grep light_tts

# æ£€æŸ¥ GPU è¿›ç¨‹
nvidia-smi pmon -s u

# é‡å¯æœåŠ¡
systemctl restart light-tts
```

#### 3. æ€§èƒ½ä¸‹é™

```bash
# æ£€æŸ¥ GPU åˆ©ç”¨ç‡
nvidia-smi dmon -s pucvmet

# æ£€æŸ¥é˜Ÿåˆ—å †ç§¯
curl http://localhost:8080/metrics | grep queue

# æ¸…ç† GPU ç¼“å­˜
python -c "import torch; torch.cuda.empty_cache()"
```

## è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬

### ä¸€é”®éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

# é…ç½®å‚æ•°
MODEL_DIR=${1:-"./pretrained_models/CosyVoice2-0.5B-latest"}
DECODE_WORKERS=${2:-3}
HTTP_WORKERS=${3:-4}

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log() {
    echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
}

error() {
    echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
    exit 1
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log "æ£€æŸ¥ä¾èµ–..."

    # æ£€æŸ¥ Python
    if ! command -v python &> /dev/null; then
        error "Python æœªå®‰è£…"
    fi

    # æ£€æŸ¥ CUDA
    if ! command -v nvidia-smi &> /dev/null; then
        error "NVIDIA driver æœªå®‰è£…"
    fi

    # æ£€æŸ¥ GPU æ•°é‡
    GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
    if [ $GPU_COUNT -lt $((DECODE_WORKERS)) ]; then
        warn "GPU æ•°é‡ ($GPU_COUNT) å°‘äº Decode Workers ($DECODE_WORKERS)"
    fi

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if [ ! -d "$MODEL_DIR" ]; then
        error "æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: $MODEL_DIR"
    fi
}

# åˆ›å»ºé…ç½®æ–‡ä»¶
create_config() {
    log "åˆ›å»ºé…ç½®æ–‡ä»¶..."

    cat > config/production.yaml << EOF
model_dir: "$MODEL_DIR"
decode_process_num: $DECODE_WORKERS
httpserver_workers: $HTTP_WORKERS
decode_max_batch_size: 3
gpt_paral_num: 150
load_trt: true
mode: ["triton_flashdecoding"]
log_level: "INFO"
EOF
}

# å¯åŠ¨æœåŠ¡
start_service() {
    log "å¯åŠ¨ Light-TTS æœåŠ¡..."

    # è®¾ç½®ç¯å¢ƒå˜é‡
    export CUDA_VISIBLE_DEVICES=0,1,2
    export OMP_NUM_THREADS=8

    # å¯åŠ¨æœåŠ¡
    ./run.sh > logs/light-tts.log 2>&1 &

    # ä¿å­˜ PID
    echo $! > /var/run/light-tts.pid

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    sleep 10

    # å¥åº·æ£€æŸ¥
    if curl -f http://localhost:8080/healthz > /dev/null 2>&1; then
        log "æœåŠ¡å¯åŠ¨æˆåŠŸï¼"
    else
        error "æœåŠ¡å¯åŠ¨å¤±è´¥"
    fi
}

# ä¸»æµç¨‹
main() {
    log "å¼€å§‹éƒ¨ç½² Light-TTS..."

    check_dependencies
    create_config
    start_service

    log "éƒ¨ç½²å®Œæˆï¼"
    log "æœåŠ¡åœ°å€: http://localhost:8080"
    log "å¥åº·æ£€æŸ¥: http://localhost:8080/healthz"
    log "ç›‘æ§æŒ‡æ ‡: http://localhost:8080/metrics"
}

main "$@"
```

### systemd æœåŠ¡é…ç½®

```ini
# /etc/systemd/system/light-tts.service
[Unit]
Description=Light-TTS Service
After=network.target

[Service]
Type=forking
User=tts
Group=tts
WorkingDirectory=/opt/light-tts
ExecStart=/opt/light-tts/scripts/deploy.sh
ExecReload=/bin/kill -HUP $MAINPID
Restart=always
RestartSec=10

# ç¯å¢ƒå˜é‡
Environment=CUDA_VISIBLE_DEVICES=0,1,2
Environment=PYTHONPATH=/opt/light-tts

# èµ„æºé™åˆ¶
LimitNOFILE=65536
LimitNPROC=4096

# æ—¥å¿—
StandardOutput=append:/var/log/light-tts/access.log
StandardError=append:/var/log/light-tts/error.log

[Install]
WantedBy=multi-user.target
```

## æ€§èƒ½åŸºå‡†

### æµ‹è¯•è„šæœ¬

```python
# test/benchmark.py
import asyncio
import aiohttp
import time
import statistics

async def benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    url = "http://localhost:8080/inference_zero_shot"

    # æµ‹è¯•æ•°æ®
    test_data = {
        "tts_text": "è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½æµ‹è¯•",
        "prompt_text": "è¯·ç”¨è‡ªç„¶çš„å£°éŸ³æœ—è¯»",
        "prompt_wav": "base64_encoded_audio_here"
    }

    async with aiohttp.ClientSession() as session:
        latencies = []

        # å¹¶å‘æµ‹è¯•
        for i in range(100):
            start_time = time.time()

            async with session.post(url, json=test_data) as resp:
                if resp.status == 200:
                    audio_data = await resp.read()
                    latency = (time.time() - start_time) * 1000
                    latencies.append(latency)

                    if i % 10 == 0:
                        print(f"Request {i}: {latency:.2f}ms")
                else:
                    print(f"Error: {resp.status}")

        # ç»Ÿè®¡ç»“æœ
        print("\n=== Performance Report ===")
        print(f"Total Requests: {len(latencies)}")
        print(f"Avg Latency: {statistics.mean(latencies):.2f}ms")
        print(f"P50 Latency: {statistics.median(latencies):.2f}ms")
        print(f"P90 Latency: {sorted(latencies)[int(len(latencies)*0.9)]:.2f}ms")
        print(f"P99 Latency: {sorted(latencies)[int(len(latencies)*0.99)]:.2f}ms")
        print(f"Min Latency: {min(latencies):.2f}ms")
        print(f"Max Latency: {max(latencies):.2f}ms")

if __name__ == "__main__":
    asyncio.run(benchmark())
```

## æ€»ç»“

æœ¬åœ°å¤š Worker éƒ¨ç½²æ–¹æ¡ˆèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å•æœºå¤š GPU èµ„æºï¼Œé€šè¿‡åˆç†çš„è¿›ç¨‹åˆ†é…å’Œå‚æ•°è°ƒä¼˜ï¼Œå¯ä»¥è·å¾—æ¥è¿‘çº¿æ€§çš„æ€§èƒ½æå‡ã€‚å»ºè®®æ ¹æ®å®é™…ä¸šåŠ¡åœºæ™¯é€‰æ‹©åˆé€‚çš„é…ç½®å‚æ•°ï¼Œå¹¶æŒç»­ç›‘æ§å’Œä¼˜åŒ–ã€‚