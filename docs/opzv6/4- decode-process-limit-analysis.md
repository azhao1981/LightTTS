# Light-TTS decode_process_num é™åˆ¶åˆ†æä¸ä¿®å¤

## ğŸš¨ é—®é¢˜æ¦‚è¿°

åœ¨ `light_tts/server/api_start.py` ä¸­å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼š

```python
# ç¬¬111-112è¡Œ
num_loras = 1
assert args.decode_process_num <= num_loras
```

è¿™ä¸ªé™åˆ¶å¯¼è‡´ `decode_process_num` æœ€å¤§åªèƒ½ä¸º 1ï¼Œä¸æ–‡æ¡£ä¸­çš„æ€§èƒ½ä¼˜åŒ–å»ºè®®ç›¸çŸ›ç›¾ã€‚

## ğŸ“Š é—®é¢˜åˆ†æ

### 1. å½“å‰å®ç°çš„é€»è¾‘

#### ç«¯å£åˆ†é…é€»è¾‘
```python
# api_start.py ç¬¬114-143è¡Œ
num_loras = 1  # ç¡¬ç¼–ç ä¸º1ï¼Œè¡¨ç¤ºåªæœ‰1ä¸ªLoRAæ¨¡å‹

# LLMç«¯å£åˆ†é…ï¼šæ¯ä¸ªæ¨¡å‹é£æ ¼1ä¸ªç«¯å£
tts_llm_ports = can_use_ports[0 : num_loras]  # åªæœ‰1ä¸ªç«¯å£

# Decodeç«¯å£åˆ†é…ï¼šä¹Ÿæ˜¯æ¯ä¸ªæ¨¡å‹é£æ ¼1ä¸ªç«¯å£
tts_decode_ports = can_use_ports[0 : num_loras]  # åªæœ‰1ä¸ªç«¯å£ï¼
```

#### è¿›ç¨‹å¯åŠ¨é€»è¾‘
```python
# api_start.py ç¬¬157-163è¡Œ
for decode_proc_index in range(args.decode_process_num):  # å°è¯•å¯åŠ¨å¤šä¸ªè¿›ç¨‹
    for style_name, tts_decode_port in zip(["CosyVoice2"], tts_decode_ports):  # ä½†åªæœ‰1ä¸ªç«¯å£
        tmp_args.append((args, tts_decode_port, httpserver_port, style_name, decode_parall_lock, decode_proc_index))
```

### 2. è®¾è®¡æ„å›¾ vs å®é™…å®ç°

| æ¦‚å¿µ | è®¾è®¡æ„å›¾ | å®é™…å®ç° | é—®é¢˜ |
|------|----------|----------|------|
| LoRA | å¤šä¸ªè¯­éŸ³é£æ ¼æ¨¡å‹ | åªæœ‰CosyVoice2ä¸€ä¸ªæ¨¡å‹ | num_lorasç¡¬ç¼–ç ä¸º1 |
| decode_process_num | Decode workerè¿›ç¨‹æ•° | è¢«é”™è¯¯é™åˆ¶ä¸ºâ‰¤1 | ä¸LoRAæ¦‚å¿µæ··æ·† |
| ç«¯å£åˆ†é… | æ¯ä¸ªæ¨¡å‹1ä¸ªç«¯å£ | å¤šè¿›ç¨‹å…±äº«1ä¸ªç«¯å£ | ç«¯å£å†²çªé£é™© |

### 3. æ–‡æ¡£ä¸ä»£ç çš„çŸ›ç›¾

**æ–‡æ¡£ä¸­çš„å»ºè®®** (docs/performance-tuning.md):
```
| RTX 4090 | 24GB | 3 workers, decode_process_num=2 | æœ€ä½³æ€§èƒ½ |
| RTX 3090 | 24GB | 3 workers, decode_process_num=2 | é«˜æ€§èƒ½ |
| A100 | 40GB | 4 workers, decode_process_num=3 | æœ€é«˜æ€§èƒ½ |
```

**ä»£ç ä¸­çš„é™åˆ¶**:
```python
assert args.decode_process_num <= num_loras  # num_loras = 1
```

**ç»“æœ**: æ–‡æ¡£å»ºè®®ä½¿ç”¨2-3ä¸ªdecode workersï¼Œä½†ä»£ç åªå…è®¸1ä¸ªï¼

## ğŸ”§ ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šæœ€å°æ”¹åŠ¨ - ä¿®å¤ç«¯å£åˆ†é…ï¼ˆæ¨èï¼‰

#### ä¿®æ”¹ç«¯å£åˆ†é…é€»è¾‘
```python
# api_start.py ä¿®æ”¹åçš„ä»£ç 
num_loras = 1
# ç§»é™¤é”™è¯¯çš„é™åˆ¶
# assert args.decode_process_num <= num_loras

# ä¸ºæ¯ä¸ªdecodeè¿›ç¨‹åˆ†é…ç‹¬ç«‹ç«¯å£
tts_decode_ports = can_use_ports[0 : args.decode_process_num]
del can_use_ports[0 : args.decode_process_num]
```

#### ä½¿ç”¨ZMQçš„PUSH-PULLè´Ÿè½½å‡è¡¡
```python
# LLM Manager (tts_llm/manager.py) ä¿®æ”¹
def __init__(self, ...):
    # ä¸ºæ¯ä¸ªdecodeç«¯å£åˆ›å»ºç‹¬ç«‹çš„socket
    self.decode_sockets = []
    for port in tts_decode_ports:
        socket = context.socket(zmq.PUSH)
        socket.connect(f"{args.zmq_mode}127.0.0.1:{port}")
        self.decode_sockets.append(socket)
        # ä½¿ç”¨è½®è¯¢è´Ÿè½½å‡è¡¡
        self.current_decode_socket = 0

def _send_to_tts2_decodec_proc(self, batch: Batch):
    for req in batch.reqs:
        if req.finish_status.is_finished():
            # è½®è¯¢å‘é€åˆ°ä¸åŒçš„decodeè¿›ç¨‹
            socket = self.decode_sockets[self.current_decode_socket]
            self.current_decode_socket = (self.current_decode_socket + 1) % len(self.decode_sockets)

            socket.send_pyobj((req.request_id, req.get_output_len()), protocol=pickle.HIGHEST_PROTOCOL)
```

### æ–¹æ¡ˆ2ï¼šä½¿ç”¨ZMQ Proxyï¼ˆæ›´ä¼˜é›…ï¼‰

#### æ·»åŠ ZMQ Proxyä½œä¸ºè´Ÿè½½å‡è¡¡å™¨
```python
# æ–°å»ºæ–‡ä»¶ï¼šlight_tts/server/zmq_proxy.py
import zmq
import threading

class ZMQProxy:
    def __init__(self, frontend_port: int, backend_ports: list):
        self.context = zmq.Context()

        # LLMè¿æ¥åˆ°frontend
        self.frontend = self.context.socket(zmq.PULL)
        self.frontend.bind(f"tcp://*:{frontend_port}")

        # Decode workersè¿æ¥åˆ°backend
        self.backend = self.context.socket(zmq.PUSH)
        for port in backend_ports:
            self.backend.bind(f"tcp://*:{port}")

        self.running = False

    def start(self):
        """å¯åŠ¨proxy"""
        self.running = True
        threading.Thread(target=self._proxy_loop, daemon=True).start()

    def _proxy_loop(self):
        """æ¶ˆæ¯è½¬å‘å¾ªç¯"""
        while self.running:
            try:
                # ä»LLMæ¥æ”¶
                message = self.frontend.recv_pyobj()
                # è½¬å‘åˆ°å¯ç”¨çš„Decode worker
                self.backend.send_pyobj(message)
            except Exception as e:
                print(f"Proxy error: {e}")

    def stop(self):
        self.running = False
```

#### ä¿®æ”¹å¯åŠ¨è„šæœ¬
```python
# api_start.py
from light_tts.server.zmq_proxy import ZMQProxy

# åœ¨å¯åŠ¨decode workersä¹‹å‰å¯åŠ¨proxy
proxy_frontend_port = can_use_ports[0]
decode_backend_ports = can_use_ports[1:1+args.decode_process_num]
del can_use_ports[0:1+args.decode_process_num]

# å¯åŠ¨proxy
proxy = ZMQProxy(proxy_frontend_port, decode_backend_ports)
proxy.start()

# LLMè¿æ¥åˆ°proxy frontend
tts_llm_to_decode_port = proxy_frontend_port

# Decode workersä½¿ç”¨å„è‡ªçš„backendç«¯å£
```

### æ–¹æ¡ˆ3ï¼šå®Œæ•´é‡æ„ï¼ˆé•¿æœŸæ–¹æ¡ˆï¼‰

#### åˆ†ç¦»LoRAå’Œworkeræ¦‚å¿µ
```python
# æ–°é…ç½®ç»“æ„
config = {
    "models": [
        {
            "name": "CosyVoice2",
            "path": "/models/CosyVoice2",
            "workers": 3  # æ¯ä¸ªæ¨¡å‹çš„workeræ•°
        }
    ],
    "load_balancing": "round_robin"  # è´Ÿè½½å‡è¡¡ç­–ç•¥
}
```

## âš ï¸ å»æ‰é™åˆ¶çš„æ½œåœ¨é—®é¢˜

### 1. ç«¯å£å†²çª
**é—®é¢˜**: å¤šä¸ªè¿›ç¨‹å°è¯•ç»‘å®šç›¸åŒç«¯å£
```bash
# é”™è¯¯ç¤ºä¾‹
Decode Process 1: bind(0.0.0.0:8083) âœ“
Decode Process 2: bind(0.0.0.0:8083) âœ— Address already in use
```

### 2. ZMQè¿æ¥é—®é¢˜
**é—®é¢˜**: å¤šä¸ªPUSHè¿æ¥åˆ°åŒä¸€ä¸ªPULLå¯èƒ½å¯¼è‡´æ¶ˆæ¯ä¸¢å¤±
```python
# é”™è¯¯çš„è¿æ¥æ–¹å¼
LLM -> PUSH -> tcp://localhost:8083
Decode1 -> PULL <- tcp://localhost:8083  # å¯ä»¥æ¥æ”¶
Decode2 -> PULL <- tcp://localhost:8083  # æ¶ˆæ¯å¯èƒ½è¢«Decode1æ¥æ”¶
```

### 3. è´Ÿè½½ä¸å‡
**é—®é¢˜**: æ¶ˆæ¯å¯èƒ½è¢«å•ä¸ªworkeræ¥æ”¶ï¼Œå…¶ä»–workerç©ºé—²

## ğŸ§ª æµ‹è¯•éªŒè¯

### éªŒè¯è„šæœ¬
```python
# test/verify_multi_decode.py
import asyncio
import aiohttp
import time
import statistics

async def test_multi_decode():
    """æµ‹è¯•å¤šdecode workeræ˜¯å¦æ­£å¸¸å·¥ä½œ"""

    # æµ‹è¯•å‚æ•°
    concurrent_requests = 20
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯å¤šworkeræ˜¯å¦æ­£å¸¸å·¥ä½œã€‚"

    async with aiohttp.ClientSession() as session:
        # å‘é€å¹¶å‘è¯·æ±‚
        tasks = []
        for i in range(concurrent_requests):
            task = send_request(session, test_text, i)
            tasks.append(task)

        # æ”¶é›†ç»“æœ
        latencies = await asyncio.gather(*tasks)

        # åˆ†æç»“æœ
        avg_latency = statistics.mean(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile

        print(f"å¹¶å‘è¯·æ±‚: {concurrent_requests}")
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
        print(f"P95å»¶è¿Ÿ: {p95_latency:.2f}ms")

        # æ£€æŸ¥GPUåˆ©ç”¨ç‡
        print("\næ£€æŸ¥GPUåˆ©ç”¨ç‡...")
        print("å¦‚æœæœ‰å¤šä¸ªdecode workersï¼Œåº”è¯¥çœ‹åˆ°å¤šä¸ªGPUè¿›ç¨‹")

async def send_request(session, text, request_id):
    """å‘é€å•ä¸ªè¯·æ±‚"""
    start_time = time.time()

    # æ„é€ è¯·æ±‚
    files = {
        "prompt_wav": ("sample.wav", open("../cosyvoice/asset/zero_shot_prompt.wav", "rb"), "audio/wav")
    }
    data = {
        "tts_text": text,
        "prompt_text": "å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚",
    }

    async with session.post("http://localhost:8080/inference_zero_shot", files=files, data=data) as resp:
        await resp.read()
        return (time.time() - start_time) * 1000

if __name__ == "__main__":
    asyncio.run(test_multi_decode())
```

### ç›‘æ§è„šæœ¬
```bash
#!/bin/bash
# monitor_decode_workers.sh

echo "ç›‘æ§Decode Workers..."
echo "================================"

# æ£€æŸ¥è¿›ç¨‹æ•°
echo "1. æ£€æŸ¥è¿›ç¨‹æ•°:"
ps aux | grep "tts_decode" | grep -v grep | wc -l | xargs echo "  Decodeè¿›ç¨‹æ•°:"

# æ£€æŸ¥ç«¯å£ä½¿ç”¨
echo -e "\n2. æ£€æŸ¥ç«¯å£ä½¿ç”¨:"
netstat -tlnp 2>/dev/null | grep :808 | head -10

# æ£€æŸ¥GPUè¿›ç¨‹
echo -e "\n3. æ£€æŸ¥GPUè¿›ç¨‹:"
nvidia-smi pmon -s u -c 1

# æ£€æŸ¥æ—¥å¿—
echo -e "\n4. æœ€è¿‘çš„é”™è¯¯æ—¥å¿—:"
tail -n 20 logs/light-tts.log 2>/dev/null | grep -i error || echo "  æ— é”™è¯¯æ—¥å¿—"
```

## ğŸ“‹ ä¿®å¤æ£€æŸ¥æ¸…å•

### ç«‹å³ä¿®å¤ï¼ˆç´§æ€¥ï¼‰
- [ ] ç§»é™¤ `assert args.decode_process_num <= num_loras`
- [ ] ä¿®æ”¹ç«¯å£åˆ†é…é€»è¾‘ï¼Œä¸ºæ¯ä¸ªdecodeè¿›ç¨‹åˆ†é…ç‹¬ç«‹ç«¯å£
- [ ] æ›´æ–°LLMåˆ°Decodeçš„å‘é€é€»è¾‘ï¼Œæ”¯æŒå¤šç«¯å£è½®è¯¢

### çŸ­æœŸä¼˜åŒ–ï¼ˆ1å‘¨å†…ï¼‰
- [ ] å®ç°ZMQ Proxyè´Ÿè½½å‡è¡¡å™¨
- [ ] æ·»åŠ decode workerå¥åº·æ£€æŸ¥
- [ ] ä¿®å¤æ–‡æ¡£ä¸ä»£ç çš„ä¸ä¸€è‡´

### é•¿æœŸé‡æ„ï¼ˆ1ä¸ªæœˆå†…ï¼‰
- [ ] åˆ†ç¦»LoRAæ¨¡å‹å’Œworkeræ¦‚å¿µ
- [ ] å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡ç­–ç•¥
- [ ] æ·»åŠ è‡ªåŠ¨æ‰©ç¼©å®¹æ”¯æŒ

## ğŸ¯ æ¨èçš„ä¿®å¤æ­¥éª¤

### ç¬¬ä¸€æ­¥ï¼šç«‹å³ä¿®å¤ï¼ˆ5åˆ†é’Ÿï¼‰
```python
# ä¿®æ”¹ api_start.py ç¬¬142-143è¡Œ
# åŸä»£ç ï¼š
tts_decode_ports = can_use_ports[0 : num_loras]
del can_use_ports[0 : num_loras]

# ä¿®æ”¹ä¸ºï¼š
tts_decode_ports = can_use_ports[0 : args.decode_process_num]
del can_use_ports[0 : args.decode_process_num]
```

### ç¬¬äºŒæ­¥ï¼šä¿®å¤LLMå‘é€é€»è¾‘ï¼ˆ30åˆ†é’Ÿï¼‰
åœ¨ `tts_llm/manager.py` çš„ `_send_to_tts2_decodec_proc` æ–¹æ³•ä¸­å®ç°è½®è¯¢å‘é€ã€‚

### ç¬¬ä¸‰æ­¥ï¼šæµ‹è¯•éªŒè¯ï¼ˆ10åˆ†é’Ÿï¼‰
è¿è¡ŒéªŒè¯è„šæœ¬ï¼Œç¡®ä¿å¤šworkeræ­£å¸¸å·¥ä½œã€‚

### ç¬¬å››æ­¥ï¼šæ›´æ–°æ–‡æ¡£ï¼ˆ5åˆ†é’Ÿï¼‰
ä¿®æ­£ `docs/performance-tuning.md` ä¸­çš„è¯´æ˜ã€‚

## ğŸ“ æ€»ç»“

è¿™ä¸ªé™åˆ¶æ˜¯ä¸€ä¸ª**å†å²é—ç•™é—®é¢˜**ï¼ŒåŸæœ¬è®¾è®¡æ”¯æŒå¤šLoRAæ¨¡å‹ï¼Œä½†ç°åœ¨å®é™…éœ€æ±‚æ˜¯å•æ¨¡å‹å¤šworkersã€‚ä¿®å¤è¿™ä¸ªé—®é¢˜å¯¹äºå‘æŒ¥å¤šGPUæ€§èƒ½è‡³å…³é‡è¦ã€‚

**ä¿®å¤åå¯ä»¥å®ç°çš„æ€§èƒ½æå‡**ï¼š
- RTX 4090: ä»1ä¸ªworker â†’ 3ä¸ªworkersï¼Œæ€§èƒ½æå‡2-3å€
- A100: ä»1ä¸ªworker â†’ 4ä¸ªworkersï¼Œæ€§èƒ½æå‡3-4å€
- å……åˆ†åˆ©ç”¨æ‰€æœ‰GPUèµ„æºï¼Œé¿å…GPUé—²ç½®

**å½±å“èŒƒå›´**ï¼š
- ä»…éœ€ä¿®æ”¹3ä¸ªæ–‡ä»¶
- ä¸å½±å“APIæ¥å£
- å‘åå…¼å®¹

è¿™æ˜¯ä¸€ä¸ª**é«˜ä¼˜å…ˆçº§**çš„ä¿®å¤ï¼Œå»ºè®®ç«‹å³å®æ–½ã€‚